**Final Technical Report:** 

**People's Voice Survey Wave 2 - Healthcare in Romania**

- Adult & Adolescent Samples -

**Final Technical Report:** 

**People's Voice Survey Wave 2 - Healthcare in Romania**

- Adult & Adolescent Samples -




Date: December 30, 2025
Prepared for: The World Health Organization (WHO)
Prepared by: MKOR




This report provides a comprehensive overview of the methodology, data collection procedures, quality control procedures and weighting strategy for the QuEST People’s Voice Survey (PVS) Wave 2 conducted in Romania. 

The primary objective of this report is to document the research process in full, ensuring transparency and providing the necessary technical details for data interpretation and future comparability. 

The fieldwork was conducted by MKOR on behalf of the World Health Organization (WHO).

**Contents**

1.  **Executive Summary: People's Voice Survey (PVS) Wave 2 - Romania****5**

1.1 Core Methodology and Scope 5

1.2 Sample Groups 5

1.3 Key Findings from Pilot and Fieldwork 6

1.4 Strategic Recommendations for Wave 3 6

1.  **Small-Scale Pilot Survey****7**

Length of Survey (LoS) 10

1.  **Main Data Collection – People’s Voice Survey - Wave 2****13**

3.1 Sample 13

3.1.1 Target Population & Scope Adjustment 13

3.1.1.a The Adult Sample 14

3.1.1.b The Adolescent Sample 14

3.1.2 Sampling Frame and Panel Construction 15

3.2 Fieldwork & Sampling Approach 15

3.2.1 Fieldwork Metrics & Response Rate 15

3.2.2 Invitation and Reminder Protocol 16

3.2.3 Quota Management 17

3.2.4 Specialized Approach: Adolescent Oversample (15-19 years) 17

1.  **Survey****20**

4.1 Translations 20

4.2 Survey Structure 20

4.3 Length of Survey (LoS) & Completion Time 23

1.  **Data Validation Methodology & Protocol****25**

5.1 Pre-Fieldwork Quality Control 25

5.2 Formal Data Validation Protocol 25

5.3 Data Cleaning & Validation of Results 26

5.4 In-Fieldwork Automated & Manual Validation 26

5.5 Post-Fieldwork Data Cleaning & Verification 27

1.  **Weighting Methodology****28**

6.1 Introduction to Weighting 28

6.2 Post-Stratification Weighting 28

6.2.1 Adults Sample: Unweighted, Weighted and INS Target Comparison 29

6.2.2 Adolescents Sample: Unweighted, Weighted and INS Target Comparison 31

1.  **Recommendations for Wave 3****33**
2.  **Final Deliverables****36**
3.  **Project Team and Roles****37**
4.  **Our Standards and Accreditations****38**
5.  **References****39**

**Appendix 1 - Small-Scale Pilot Survey****42**

**Appendix 2 - Adults Sample****45**

**Appendix 3 - Adolescent Sample****49**







# **1\. Executive Summary: People's Voice Survey (PVS) Wave 2 - Romania**

This report details the execution of the People's Voice Survey (PVS) Wave 2 in Romania, conducted by MKOR on behalf of the World Health Organization (WHO). The project’s scope and objective were to collect high-quality, nationally representative data regarding the Romanian healthcare system from two distinct populations, for further detailed analysis and report. The report covers the collection of two data sets: a nationally representative sample of Romanian adults and a crucial oversample of adolescents.

Rigorous data quality protocols were enforced at every stage, and a comprehensive post-stratification weighting scheme was applied to both samples to ensure the final dataset is nationally representative and comparable with Wave 1. This report documents all methodological challenges, solutions, fieldwork procedures and quality assurance measures for both sample groups, confirming the delivery of a reliable and high-integrity dataset.

## 1.1 Core Methodology and Scope

**Data Collection Period**: Main fieldwork took place from November 21 to December 16, 2025.

**Methodology**: The study utilized Computer-Assisted Web Interviewing (CAWI) via MKOR's proprietary national online panel of over 80,000 members.

## 1.2 Sample Groups

*   **Adults (18+)**: Achieved a final validated sample of 2,000 individuals.
*   **Adolescent Oversample (15-19)**: Achieved a target of 500 valid completes. Participants were recruited using a combination of panel recruitment and outreach strategies involving youth organisations. However, this group proved to be hard to reach, due to lower response rates and reticence towards comprehensive surveys. The age range was strategically expanded from an initial 15-17 band to 15-19 years, to ensure a robust dataset, and aligns with the WHO age classification for adolescents. 

**Weighting**: Post-stratification weighting (iterative proportional fitting) was applied to both samples using National Institute of Statistics (INS) data to ensure national representativeness and comparability with Wave 1.

## 1.3 Key Findings from Pilot and Fieldwork

**Pilot Success**: A small-scale pilot (N=100) confirmed the instrument's clarity, with 92% of respondents finding questions easy to understand and 94% finding the survey length appropriate.

**Respondent Experience**: The median completion time for the main survey was 17 minutes (excluding outliers).

**Quality Control**: Rigorous protocols were enforced, including automated screening for logical consistency and manual reviews for "speeding" (completing in under 7 minutes). A total of 874 responses were invalidated during the main collection phase due to incompleteness or quality flags.

## 1.4 Strategic Recommendations for Wave 3

MKOR suggests several improvements for future iterations, including:

*   **Defining Adolescents as 15-19**: Formally adopting this wider range from the start to capture the full "older adolescent" experience.
*   **Modular Design**: Implementing a "Core + Satellite" questionnaire to reduce respondent fatigue and improve data quality.
*   **Mixed-Mode Collection**: Reintroducing telephone (CATI) or in-person (CAPI) modules for the 65+ demographic to improve raw data quality for older cohorts.
*   **Qualitative Component**: Adding in-depth interviews to provide "the why" behind the quantitative trends.

# **2\. Small-Scale Pilot Survey**

A pilot survey for the "People's Voice Survey Wave 2" was conducted to test the general survey infrastructure, technical performance of the survey instrument and generate insights into the administration of critical survey questions on the healthcare system so that essential changes can be implemented before the full survey roll-out. The objective of the pilot survey was to undertake a post-fieldwork data review to validate the survey's technical performance and data output and test the survey instrument's technical functionality, clarity and flow with an engaged audience. This process involved a line-by-line analysis of the entire dataset. The data validation team meticulously examined each question as programmed in the survey, alongside the full spectrum of answers received from the respondents. By cross-referencing the intended logic with the actual response data, we were able to effectively identify inconsistencies in programming, translation and response options. This granular level of review was essential for diagnosing the issues detailed in the 'Problems Identified' section and ensuring all necessary corrections were made before full fieldwork commencement.

2.1 Methodology

**Pilot sample**

The pilot was conducted on a sample of N=100 residents of Romania aged 15 and over, as per the WHO requirements, whose demographic characteristics. To achieve this target, an initial sample of 1,480 panelists were invited. Invitations were sent to a random selection of MKOR Panel members. 

*   Pilot Sample - Gender




*   **Pilot Sample - Age**

**Invitation and Reminders**

The Invitation text was generic, on “health and life perceptions”, with no specific information that could bias survey entry. Pilot participants were informed in advance that completion of the survey was incentivised through participation in a raffle draw with a monetary prize of 1500 RON, in line with MKOR’s standard panel practices, to acknowledge respondent time and encourage engagement. A single email reminder was sent to non-respondents two days after the initial invitation to maximize participation. 

**Indicators** 

To evaluate the survey’s performance during the pilot phase, we examined the **pilot data quality**, **survey length**, as well as the **experience of the pilot respondents**.

2.2 Pilot Key Findings 

**Pilot data quality**

To ensure the highest level of data integrity, the validation process combined automated algorithms with human oversight to identify and exclude invalid responses to secure a final, validated sample of 100 interviews  for the Pilot phase.

For this, we used the following validation criteria:

1.      **Full Completion:** 4 incomplete responses were invalidated.
2.      **Logical Coherence:** 6 response inconsistencies were automatically and manually human-validated.   Our automated screening is programmed to flag respondents based on logical inconsistencies and a lack of attention. For example, the system automatically invalidates responses containing contradictory data (e.g., visiting more facilities than the total number of visits reported) or nonsensical open-ended text (e.g., "test"). It also flags and invalidates inattentive behavior like "straight-lining," where a user provides the same answer to all statements in a grid.
3.    **Speed Check**: The data validation team conducted manual data checks, primarily by filtering the database to identify "speedsters" (respondents who complete the questionnaire in an unrealistically short amount of time, under 7 minutes).



In this context, "Survey Starts" (N=110) represent individuals who began the questionnaire. The difference between this number and the "Final Valid Completes" (N=100) is composed of two distinct groups: respondents who dropped off mid-survey ("break-offs") and respondents who completed the survey but were invalidated during the quality control process.

These figures produced the key performance indicators that validated the health of the survey instrument:

*       **Response Rate**: 7.4% of those invited started the survey, indicating a healthy level of interest from the target audience.


*   **Completion & Quality Rate**: A remarkably high 90.9% of individuals who started the survey proceeded to complete it and pass our rigorous automated and manual data validation protocols.

The high conversion rate from "Starts" to "Final Valids" is a strong testament to the questionnaire's design. It demonstrates that the instrument was engaging, clear and effectively minimised respondent fatigue and drop-offs. The 10 surveys that were initiated but did not become part of the final sample were removed due to a combination of respondent drop-offs and our quality control screenings.

These strong performance metrics confirmed the survey's readiness for a successful main-wave launch.

## **Length of Survey (LoS)** 

A critical objective of the pilot phase involved conducting a temporal analysis of the survey completion data. This diagnostic assessment was designed to evaluate the instrument's performance and establish robust data validation protocols for the main fieldwork. Our analysis of the 100-respondent pilot sample revealed an average completion time of approximately 36 minutes. Further analysis of the distribution showed a normative completion window, with the interquartile range (representing the 25th to 75th percentiles) falling between 12 and 21 minutes. This concentration of response times indicates a consistent and focused experience among the core participant group. The analysis also identified significant temporal outliers, with some response durations extending beyond 20 hours.

This finding is consistent with the anticipated respondent engagement time for a research instrument of this scope and complexity, confirming that its length and cognitive load are calibrated for the target population.

In light of these findings during the Pilot phase, the following data validation protocols were established for the main data collection wave to ensure the integrity of the final dataset:

*   **Central metric**: The median was formally adopted as the primary key performance indicator (KPI) for reporting survey duration, as it is robust against the influence of extreme outliers.
*   **Upper-bound criterion**: A programmatic filter was implemented to exclude all cases with a total completion time exceeding 60 minutes. This threshold systematically removed non-representative "device idle" data points from the time analysis.
*   **Lower-bound flag**: Respondents with completion times below 7 minutes were automatically flagged for manual review. This threshold was designed to identify accelerated completion patterns potentially indicating survey satisficing or fraudulent behaviour, warranting further inspection of data quality.

**Pilot Respondent Experience Analysis**

The feedback from the 100 pilot valid participants was **very positive**, confirming the questionnaire's readiness for a full launch.

The instrument performed well across key metrics of respondent experience.

*   **Perceived Length**: The vast majority of respondents (94%) found the survey's duration to be appropriate. Specifically, 63% rated the length as "Acceptable," while another 31% found it "Short" or "Very short." Only 6% of participants perceived it as "Long," indicating that respondent fatigue is not a significant concern.
*   **Clarity and Comprehension**: The questions were very well understood by the participants. An impressive 92% of respondents found the questions "Very easy" (60%) or "Easy" (32%) to understand. This high level of clarity suggests the wording is effective and accessible across different demographic groups.
*   **Comfort with Topics**: Despite covering potentially sensitive health topics, the questionnaire was well-received. A combined 86% of respondents reported feeling "Very comfortable" (45%) or "Comfortable" (41%) with the subject matter. The remaining 14% found the topics "Acceptable," with no respondents reporting feeling uncomfortable. This indicates the survey navigates sensitive areas appropriately.

2.3 Resolved Issues

The pilot was successful in confirming the instrument's overall readiness while identifying several key areas for refinement.  Respondent feedback was overwhelmingly positive, with 92% finding the questions "Easy" or "Very Easy" to understand and 94% finding the length appropriate.

The pilot was instrumental in uncovering and resolving the following issues, which were all corrected before the main fieldwork commenced:

*   **Question Omission**: A programming error that prevented a question on cervical cancer screenings from displaying was identified and corrected.
*   **Missing Follow-up**: A necessary follow-up question regarding the type of telemedicine services used was added to the instrument.
*   **Ambiguous Translation**: A question about seeking healthcare outside of Romania was retranslated for clarity ("_În ce țară sau țări ai beneficiat de servicii medicale în ultimele 12 luni, excluzând România?_") to prevent misinterpretation.
*   **Incorrect Response Options**: An "Other (please specify)" option was incorrectly included on two rating-scale questions and was subsequently removed.
*   **Incorrect Display Logic**: The Net Promoter Score (NPS) question logic was adjusted to ensure it was only shown to respondents who had a healthcare visit in the last 12 months, in order to address potential recency bias.

All issues identified during the pilot, both through data analysis and subsequent requests from the WHO team, were fully resolved before the commencement of the main fieldwork, ensuring the instrument's readiness.




# **3\. Main Data Collection – People’s Voice Survey - Wave 2**

This chapter provides a detailed overview of the methodology, challenges and outcomes for the main data collection phase, covering both the adult and adolescent samples.

## 3.1 Sample

### 3.1.1 Target Population & Scope Adjustment

The study was designed to capture insights from two distinct target populations residing in Romania:

1.  **Main Target Population**: All individuals aged 18 and over
2.  **Adolescents Target Population**: An oversample of adolescents aged 15-17

The objective was to generate a dataset that could withstand external scrutiny, requiring high population coverage with reasonably uniform representation across key demographic variables for both groups (**Appendices 2 and 3**).

For Wave 2, a Computer-Assisted Web Interviewing (CAWI) methodology was implemented. CAWI is a quantitative research methodology where respondents complete a survey or interview via the internet using a computer, tablet, or smartphone. Unlike traditional methods, which require an interviewer to ask questions, CAWI is efficient because it eliminates the need for trained interviewers, travel, or physical materials to target large numbers and is self-administered. The respondent follows a dynamic script provided on a website or mobile app, and their answers are automatically saved to a database in real-time.

This online approach was deemed optimal for reaching both — target groups. For the adult sample, high internet penetration rates ensure that a sample drawn from a large national panel can be weighted to be representative. 

For the adult sample, high internet penetration rates ensure that a sample drawn from a large national panel can be weighted to be representative. 

For the adolescent sample, the online methodology, supplemented with targeted youth communities outreach, provided the most effective and ethically sound means of engagement. This dual approach ensures that the final datasets for both populations are robust and representative.

#### **3.1.1.a The Adult Sample**

The adult target population is defined as 18 and over. Our online panel's effective reach and the stratified sampling methodology are optimized to ensure robust representation to the age of 65. 

Individuals over this age have significantly lower online panel feasibility and while not explicitly excluded, their incidence in the final sample is expected to be minimal. This approach was aligned with the WHO as part of the project specifications to ensure a high-quality, digitally accessible sample.

#### **3.1.1.b The Adolescent Sample**

The initial project scope targeted an oversample of 500 individuals aged 15-17 years. During the fieldwork phase, significant challenges were encountered in engaging this specific, narrow age band, well-documented due to lower response rates and reticence towards comprehensive surveys. 

To ensure the critical objective of capturing younger voices was met, a methodological decision was made in agreement with the WHO team to broaden the target age range to 15-19 years old as per the WHO classification of adolescents. This adjustment to include 18-19 year-olds allowed for the successful achievement of the N=500 target and provided a robust dataset covering the full adolescent experience.

### 3.1.2 Sampling Frame and Panel Construction

The sampling approach for the adult population was designed to achieve a nationally representative sample of 2,000 individuals aged 18 and over. A stratified sampling methodology was used, with population targets based on the latest census data from the Romanian National Institute of Statistics (INS).

The sampling frame for this study was MKOR's proprietary national online panel, which comprises over 80,000 active and validated members across Romania. The panel is a 100% opt-in research community. Panelists are recruited through a diverse range of verified digital channels, including targeted social media campaigns, partnerships with reputable online publishers and member referrals.

Each panelist completes a demographic questionnaire upon registration, which includes key variables such as age, gender, region (NUTS2), residence (urban/rural) and education level. This information is regularly updated and is used to ensure the selection of nationally representative samples. All panel activities are conducted in full compliance with GDPR.

GDPR compliance is embedded across the full lifecycle of panel management and data collection. MKOR’s GDPR framework has been developed and is continuously reviewed in collaboration with an external senior GDPR advisor. All panel members provide explicit, informed consent at registration and reaffirm their consent at the start of each survey via the survey platform. Participants are informed about the purpose of the research, data usage, storage duration, and their rights under GDPR, including the right to withdraw, access, rectify, or delete their data at any time. Personal data are processed on secure servers, access is restricted to authorized personnel only, and all analyses are conducted on anonymized datasets, ensuring that no individual respondent can be identified in reporting or data outputs.

Invitations were sent to a targeted selection of MKOR panel members to align with quotas for key demographic variables, including Region (NUTS 2), Gender, Age and Residence (Urban/Rural). These quotas were actively managed throughout the fieldwork period to ensure the final sample structure mirrored the official population distribution, guaranteeing a high degree of representativeness.

## 3.2 Fieldwork & Sampling Approach

### 3.2.1 Fieldwork Metrics & Response Rate

A stratified sampling approach was implemented to ensure the final sample was statistically representative of the Romanian population.

A total of **22,684 unique individuals** were randomly invited to participate in the adult survey. Responses were followed in real time and follow-up invitations were sent to targeted demographics, aimed at filling the quotas, as per the INS distribution.

The fieldwork for the adult main sample (N=2,000) yielded the results presented in Appendix 2.

**Responsiveness: Demographic Distribution**

The analysis comparing the invited sample to the final unweighted sample revealed several typical patterns for online panel research:

*   **Age:** Younger demographics (18-34) showed a higher propensity to complete the survey compared to their proportion in the invited sample.
*   **Gender:** More females participated in and completed the survey than males.
*   **Education:** Individuals with high education were over-represented in the final valid sample compared to their share of the invited group.
*   **Residence (urban and rural)**: More individuals from the urban areas participated in the survey, an expected outcome in online surveys.

These observed deviations are the primary reason for implementing a rigorous post-stratification weighting scheme. The weighting process directly corrects for these non-response patterns by adjusting the final sample to match the known population structure from the INS census data.

### 3.2.2 Invitation and Reminder Protocol

An initial email invitation containing a unique personalized survey link was sent to all selected panelists. To maximize response rates, a reminder campaign was implemented. The reminder was sent via email two days after the initial invitation to all panelists who had not yet started the survey. No individual received more than one reminder. As per our standard practice for the research panel, participants were offered an incentive (participation in a raffle draw with two monetary prizes, of 1500 RON, one for each target sample - adult & adolescents) for their time and contribution.

### 3.2.3 Quota Management

Fieldwork progress was monitored in real-time through an automated dashboard. Quotas for key demographic variables (age, gender, region, urban/rural) were actively monitored and managed. As certain quotas were filled, invitations were strategically directed towards under-represented groups to achieve a balanced and representative final sample.

### 3.2.4 Specialized Approach: Adolescent Oversample (15-19 years)

**Challenge**: Lower engagement rates and reticence towards longer questionnaires, a phenomenon noted in survey methodology literature. While our pilot study indicated a positive respondent experience among the few adolescents who participated, scaling this to a large, representative sample of 500 completes within the narrow 15-17 age band proved challenging during the initial fieldwork phase.

**Multi-Phase Recruitment Strategy**

To overcome these challenges, a dynamic, multi-phase strategy was designed and implemented in close collaboration with the WHO team.

**Phase 1: Standard Panel Invitation and Reminder Protocol**

The initial recruitment efforts focused on our proprietary panel and included:

*   **Direct Panel Invitations (Adolescents 16–17 and Parents of 15-Year-Olds):** Direct invitations were sent to panel members aged 16–17, who are able to provide informed consent independently, as well as to adult panel members identified as parents or legal guardians of 15-year-old adolescents, in line with consent requirements.
*   **Targeted Pre-Screeners:** Invitations were sent to adult panel members pre-profiled as parents of adolescents within the expanded 15–19 age range, enabling targeted recruitment of eligible respondents.
*   **Youth-Adapted Communications:** Invitation materials were specifically designed to be visually engaging and age-appropriate, with language and formatting tailored to resonate with adolescent respondents.
*   **SMS Invitations:** Mobile-first SMS reminders were deployed for adolescents aged 16 and over, for whom independent consent applies, to increase invitation visibility and improve response rates.

**Phase 2: Approved Methodological Enhancement & Scope Adjustment**

After the initial fieldwork week, it became evident that the response rate from the 15-17 age group alone was insufficient to reach the target of N=500 within the project timeline. In response, two solutions were discussed and approved with the WHO team:

*   **Scope Adjustment**: The target age range for the adolescent oversample was broadened from 15-17 to 15-19 years old, which aligns with the WHO classification. This decision was made to ensure the final sample was robust and included the voices of mid to late adolescents (18-19), who serve as a close proxy and are contextually part of the same cohort. This action served as the project's primary contingency plan.
*   **Community Outreach**: To supplement the panel recruitment, a targeted outreach program was initiated via national youth organizations. This approach leveraged the trust of local community leaders to mobilize adolescent respondents who might not be part of a traditional research panel. Each organisation was provided with a unique, trackable survey link, enabling source-level monitoring and validation of incoming responses. To safeguard data integrity and prevent duplicate participation, the survey platform applied technical controls including unique link enforcement and system-level restrictions designed to limit multiple survey commencements from the same device or IP address. These measures were complemented by post-fieldwork validation checks to identify and remove any residual duplicate or invalid cases.

**Methodological Safeguards and Validation**: To mitigate potential sampling biases from the non-panel community outreach, the following rigorous safeguards were implemented:

*   **Unique Survey Links**: Each partner organization received a unique, trackable link to monitor the source of respondents.
*   **Continuous Sample Validation**: The demographic profile of participants from community channels was continuously validated against the profile of panel-recruited adolescents to detect any significant skews.
*   **Post-Stratification Weighting**: To ensure the final adolescent dataset was balanced and representative, a post-stratification weighting process was applied. The sample was calibrated to align with INS population targets for age, gender and residency, correcting for any imbalances that arose from the multi-phase recruitment strategy.

# **4\. Survey** 

The questionnaire used in Romania was based on the main QuEST tool, based at the University of Washington. It has been carefully revised by the WHO and adjusted to better align with the local context.

Following review of the pilot data, the changes to the questionnaire for the main fieldwork were agreed. The amendments were implemented by MKOR and reviewed and signed off by the WHO. Once the final version of the script was signed off, the teams proceeded to update the translations.

## 4.1 Translations 

The development of the final Romanian questionnaire was a multi-stage, collaborative process designed to ensure the highest degree of clarity, cultural relevance and comprehension.

1.  **Initial Translation**: The master questionnaire, developed in English, was initially translated into Romanian by the WHO team.
2.  **Cognitive Interviews**: Following the initial translation, the WHO local team conducted 5 cognitive interviews with respondents whose recruitment was facilitated by MKOR. Insights from these interviews were used to identify and refine questions that were ambiguous or difficult to understand, leading to a revised and improved translation.
3.  **Pilot Testing & Final Review**: This updated questionnaire was then programmed for the online pilot test. Following the pilot, both the MKOR and WHO local teams conducted a final review of the instrument, incorporating learnings from the pilot's performance to produce the final version for the main fieldwork.

## 4.2 Survey Structure

The survey instrument was designed to be comprehensive yet efficient. It consisted of **103 questions in total**, including 93 closed-ended questions (single-choice, multiple-choice, grid, scale), 11 open-ended questions and 14 demographic questions. 

| **Component** | **Count** | **Included question IDs (examples)** | **Notes** |
| --- | --- | --- | --- |
| **Total questions (response fields)** | **103** | INTRO, Q2–Q78, Q80–Q102 | Q79 is story text (no response), so not counted as a question |
| **Closed-ended questions (total)** | **92** | single-choice, multi-choice, grid, scale | Includes all categorical, grids and scales |
| └─ Single-choice (incl. numeric categorical lists) | 79 | e.g., Q3, Q6, Q11, Q37, Q58 | Includes “how many visits” lists, Yes/No, ratings, etc. |
| └─ Multiple-choice | 7 | Q7, Q41, Q43, Q49, Q50, Q96, Q100 | “Select all that apply” |
| └─ Grid / matrix | 5 | Q13, Q19, Q45, Q57, Q73 | Each grid counted as 1 question |
| └─ Scale (0–10) | 1 | Q76 | NPS-style |
| **Open-ended questions (total)** | **11** | Q2, Q48, Q54, Q61, Q64, Q97, Q98, Q46\_mental\_1, Q101, Q102 | Comment box |
| **Demographic / profiling questions** | **14** | Q2–Q10; Q96–Q100 | Includes age, gender, county, settlement type, insurance coverage/type, education (current + completed), language, income, household size, children (and their ages) |




The survey follows a shared core questionnaire for all respondents, complemented by age-specific modules: adolescents (15–19) receive additional questions on school wellbeing, mental-health communication, confidentiality and HPV vaccination, while adults (18+) receive expanded modules on vaccination attitudes, financial hardship related to healthcare and detailed socioeconomic profiling.

| **Adults (18+ years)** | **Adolescents (15–19 years)** |
| --- | --- |
| Consent & eligibility • Informed consent • Age verification (18+) | Consent & eligibility • Informed consent • Age verification (15–19) |
| Core demographics • Gender • County & settlement type • Education completed • Insurance coverage & payment source | Core demographics • Gender • County & settlement type • Education (current + completed) • Insurance coverage & payment source |
| General health status • Self-rated physical health • Self-rated mental health • Mental health symptom frequency (depression/anxiety screener) • Long-standing illness | General health status • Self-rated physical health • Self-rated mental health • Mental health symptom frequency (depression/anxiety screener) • Long-standing illness |
| — | School-related wellbeing (students only) • Worry about school performance • Experience of bullying |
| — | Youth mental-health access & communication • Comfort discussing mental health with providers • Whether mental health was discussed in the past year • Type of provider consulted • Whether providers proactively asked about mental health, bullying, substance use, social media, sleep, violence |
| — | Confidentiality of care (15–19 only) • Ever spoke privately with a provider without parents/guardians |
| Sexual & reproductive health (age- and sex-specific) • Comfort discussing sexual health • Sexual health consultations • Provider type • Pregnancy, birth, contraception confidence (women 15–49) | Sexual & reproductive health (age- and sex-specific) • Comfort discussing sexual health • Sexual health consultations • Provider type • Pregnancy, birth, contraception confidence (women 15–49) |
| Usual source of care • Existence of a regular facility • Facility ownership & type • Reason for choice • Overall quality rating | Usual source of care • Existence of a regular facility • Facility ownership & type • Reason for choice • Overall quality rating |
| Healthcare utilization (past 12 months) • Number of in-person visits • Home care visits • Virtual / telemedicine visits • Type and quality of services used | Healthcare utilization (past 12 months) • Number of in-person visits • Home care visits • Virtual / telemedicine visits • Type and quality of services used |
| Preventive care & screenings (age/sex-specific) • Blood pressure, vision, dental, metabolic screening • Cancer screenings (breast, cervical, colorectal) | Preventive care & screenings (age/sex-specific) • Blood pressure, vision, dental, mental health care • HPV vaccination awareness & uptake |
| Vaccination perceptions & behaviour • Vaccination completeness • Refusal or delay • Future intentions • Vaccine confidence & trust (attitudinal battery) | Vaccination perceptions • HPV vaccination awareness, uptake, age at vaccination |
| Unmet need & coping • Forgone care (general, mental, sexual health) • Reasons for non-use • Informal coping • Financial coping (borrowing money, selling assets) | Unmet need & coping • Forgone care (general, mental, sexual health) • Reasons for non-use • Informal coping (family, friends, internet, hotlines) |
| Experience of care & safety (if any care used) • Last visit experience (waiting time, respect, communication, privacy) • Recommendation likelihood • Perceived medical errors • Discrimination | Experience of care & safety (if any care used) • Last visit experience (waiting time, respect, communication, privacy) • Recommendation likelihood • Perceived medical errors • Discrimination |
| Expectations of care quality • Vignette-based quality assessment | Expectations of care quality • Vignette-based quality assessment |
| Health system confidence • Trust in the system • Confidence in quality, affordability, responsiveness • Perceived system trajectory | Health system confidence • Trust in the system • Confidence in quality, affordability, responsiveness • Perceived system trajectory |
| Extended demographics & feedback • Language spoken at home • Income & household size • Children & children’s ages • Open feedback & improvement suggestions | Extended demographics & feedback • Language spoken at home • Children & & children’s ages (if applicable) • Open feedback & improvement suggestions |

## 4.3 Length of Survey (LoS) & Completion Time

The survey's completion time was closely monitored to ensure it was within an acceptable range for respondent engagement and to identify potential data quality issues.
When reporting, a programmatic upper-bound (see below) filter was implemented to exclude outliers with completion times exceeding 60 minutes, which typically represent "device idle" time rather than active engagement.

| **Fieldwork Metric** | **Value (minutes)** | **Description** |
| --- | --- | --- |
| Average LOS | 29 | The average time it took for someone to respond |
| Average LOS excluding outliers | 19 | The average time it took for someone to respond, excluding people that took more than one hour to respond |
| Median LOS excluding outliers | 17 | The middle value of all valid survey completion times, excluding people that took more than one hour to respond |
| Longest active completion time | 59 | The longest survey completion time under one hour, excluding people that took more than one hour to respond |
| Longest total completion time | 3,081 | The longest survey completion time for someone that stopped answering and resumed at a later time  |

Analysis of the completion time distribution revealed a concentration of responses within a normative window, as indicated by the interquartile range. However, the arithmetic mean was significantly inflated by a number of extreme temporal outliers.

In an online (CAWI) survey context, very long completion times are typically not indicative of a lengthy survey but rather of respondent dormancy or "device idle time," where a respondent pauses the survey and returns to it much later. As these anomalous data points do not reflect active engagement, the median was adopted as the most robust and reliable measure of central tendency for survey duration.

To ensure the integrity of the final dataset, a formal Data Validation Protocol was established based on these findings. This protocol, detailed in the following section, was systematically applied to the main fieldwork data.

# **5\. Data Validation Methodology & Protocol**

## 5.1 Pre-Fieldwork Quality Control

Before the launch of the main survey, a series of rigorous checks were conducted to maximize data quality and ensure the instrument's technical soundness:

*   **Screen-by-Screen Validation**: The programmed online survey was checked screen-by-screen against the master English and final Romanian versions of the questionnaire to ensure perfect correspondence.
*   **Logic and Routing Checks**: All pre-scripted hard and soft logic checks, including skip patterns and conditional branching, were thoroughly tested by the MKOR and WHO teams to confirm they were functioning correctly.
*   **Native Language Review**: The final programmed survey was reviewed by a native Romanian speaker to check for any linguistic or contextual issues in the live environment.

## 5.2 Formal Data Validation Protocol

In light of the diagnostic findings from the pilot phase, a formal Data Validation Protocol was established for the main data collection wave to ensure the integrity of the final dataset. This protocol, developed by our Lead Methodologist, combines automated system rules with expert manual review and was a core component of the project's quality assurance plan. The key pillars of this protocol are:

*   **Central Metric for Duration**: The median was formally adopted as the primary key performance indicator (KPI) for reporting survey duration, as it is robust against the influence of extreme "device idle" outliers.
*   **Upper-Bound Criterion**: A programmatic filter was implemented to automatically exclude all cases with a total completion time exceeding 60 minutes.
*   **Lower-Bound Flag**: Respondents with completion times below 7 minutes were automatically flagged for manual review to identify potential survey satisficing ("speeding") or fraudulent behaviour.




## 5.3 Data Cleaning & Validation of Results

The protocol described above was systematically applied during and after the fieldwork. This process combined sophisticated automated checks with expert human oversight to secure a final, validated sample of 2,000 surveys for the General Adult Population and the Oversample of 500 Adolescents..

## 5.4 In-Fieldwork Automated & Manual Validation

Our data collection platform is equipped with real-time automated screening to flag and filter out low-quality responses as they are submitted. This was supplemented by manual checks by our in house data quality team to ensure the validity of the final dataset. Based on these protocols, the following responses were invalidated and removed:

| **Invalidation Metric** | **Invalid Responses** | **Description** |
| --- | --- | --- |
| **Full Completion (Break-offs)** | **616** | Responses where the respondent dropped off before reaching the final question. | Responses where the respondent dropped off before reaching the final question. |
| **Logical Coherence** | **229** | Responses invalidated by the system and the data quality team for logical inconsistencies, inattentive behaviour, etc | Responses invalidated by the system and the data quality team for logical inconsistencies, inattentive behaviour, etc |
| **Speed Check** | **29** | Responses invalidated after manual review for answering in an unrealistically short time (under 7 minutes) | Responses invalidated after manual review for answering in an unrealistically short time (under 7 minutes) |
| **Total invalidated** | **874** | **874** | **874** |

These observed deviations are typical for online panel research and were the primary reason for implementing a rigorous post-stratification weighting scheme. The weighting process directly corrects for these non-response patterns by adjusting the final sample to match the known population structure from the INS census data.

## 5.5 Post-Fieldwork Data Cleaning & Verification

After the fieldwork was closed, the coordination team undertook a final series of checks to maximize the quality and integrity of the dataset:

*   **Frequency Checks**: Final frequency checks were run on all questions to verify the accuracy of survey routing and logic. No issues were identified.
*   **Permitted Values**: The dataset was checked to ensure that all variables contained only the pre-defined, permitted values. No issues were identified.
*   **Duplicate Records**: The dataset was checked for any duplicate or near-duplicate records using unique respondent IDs and response patterns. No duplicates were found.
*   **Impossible / Implausible Values**: The data was checked for any remaining impossible or implausible values. No such values were noted in the closed-ended data.

# **6\. Weighting Methodology**

## 6.1 Introduction to Weighting

To ensure the final survey data accurately reflects the demographic profile of the Romanian adult population and to correct for potential biases arising from sampling deviations and non-response, a post-stratification weighting process was applied. This process adjusts the achieved sample to align with known population parameters.

## 6.2 Post-Stratification Weighting

Calibration weights were calculated to adjust the final sample of 2,000 adults to match the known population parameters of Romania. The population data was based on the latest available data from the National Institute of Statistics (INS), as of July 2025. Weights were calculated to align with the methodology from Wave 1, calibrated for age band, gender, formal education level, and Residence (rural/urban).

An iterative proportional fitting algorithm was used to calculate the weights. This process ensures that the marginal distributions of these variables in the survey data match the official population statistics, thus enhancing the reliability and generalizability of the findings.

**Weighting Variables**: In alignment with the methodology from Wave 1, the sample was calibrated for the following variables:

*   Age Band
*   Gender
*   Formal Education Level
*   Residence (Urban / Rural)

**Weighting Algorithm**: An iterative proportional fitting algorithm (raking) was used to calculate the weights. This process ensures that the marginal distributions of the weighting variables in the survey data match the official INS population statistics, thus enhancing the reliability and generalizability of the findings.

### 6.2.1 Adults Sample: Unweighted, Weighted and INS Target Comparison

The following graphs illustrate the demographic profile of the final adult sample before and after weighting, demonstrating the adjustments made to achieve national representativeness.

*   **Adult Sample - Gender**

*   **Adult Sample - Age**

*   Adult Sample - Residence

*   **Adult Sample - Education**

### 6.2.2 Adolescents Sample: Unweighted, Weighted and INS Target Comparison

The following graphs illustrate the demographic profile of the final adolescents sample before and after weighting, demonstrating the adjustments made to achieve national representativeness.

*   **Adolescents Sample - Gender**




1.  **Adolescents Sample - Age**

1.  **Adolescents Sample - Residence**




# **7\. Recommendations for Wave 3**

Based on the successful execution and methodological learnings from Wave 2, MKOR offers the following strategic recommendations for the World Health Organization to consider in the planning and design of future People's Voice Survey waves. These suggestions are intended to enhance the project's feasibility, data richness, and policy impact.

**Formalize the Adolescent Sample as 15-19 in Future Project Scopes**

*   **Observation:** The primary methodological challenge of this wave was achieving a robust sample within the narrow 15-17 age band, which necessitated a strategic scope adjustment to 15-19 years old in collaboration with the WHO team.
*   **Recommendation for WHO:** To avoid the need for mid-fieldwork contingency plans, we recommend officially defining the adolescent target population as **15-19 years old** from the outset in the project's terms of reference covering mid to late adolescence. This sets realistic fieldwork expectations and captures the full "older adolescent" experience, providing a more comprehensive dataset for policy analysis.

**Adopt a Modular Questionnaire Design to Enhance Data Quality**

*   **Observation:** The comprehensive nature of the PVS questionnaire is a significant driver of respondent fatigue, as evidenced by the drop-out rate and the need for rigorous data cleaning (e.g., for "speeding").
*   **Recommendation for WHO:** We recommend exploring a **modular "Core + Satellite" design** for Wave 3. This would involve:

*   A **"Core" module** of essential longitudinal questions asked of every respondent for tracking purposes.
*   **Rotating "Satellite" modules** on specific, in-depth topics (e.g., mental health, cancer screenings, vaccination confidence) asked of specific sub-samples.
*   This approach would maintain crucial trend data while reducing the length and cognitive load of any single survey, thereby improving respondent engagement and overall data quality.

**Improve Representation of Older Cohorts (65+) via Mixed-Mode & Proxy Reporting**

*   **Observation:** Online panels inevitably skew younger and more educated. While weighting corrects for this, reaching the 65+ demographic online remains a challenge, leading to a reliance on heavier statistical weighting for this group.
*   **Recommendation for WHO:** To improve the raw data quality for older age bands, we recommend:

*   **Reintroducing CATI/CAPI:** Complementing the online survey with a targeted telephone or in-person module specifically for the 65+ age band.
*   **Allowing Proxy Reporting:** Implementing a protocol that allows family members or caregivers to provide certain non-perceptual information for very elderly relatives who may have difficulty completing the survey themselves.

**Targeted Recruitment for "Hard-to-Reach" Socio-Demographics**

*   **Observation:** Similar to the challenges with the adolescent cohort, low-education demographics are harder to reach via standard online panel invitations.
*   **Recommendation for WHO:** Future waves should implement **specialized recruitment strategies** for low-education demographics, similar to the successful youth organization outreach used in Wave 2. This proactive targeting will improve raw sample balance and further reduce the reliance on post-stratification weighting.

**Continue to Ensure a Mobile-First Instrument Design**

*   **Observation:** Younger cohorts (adolescents) were successfully reached via SMS reminders, meaning they are accessing the survey almost exclusively via smartphones.
*   **Recommendation for WHO:** Continue to ensure the survey instrument is fully optimized for mobile responsiveness. A mobile-first design philosophy is critical to reducing attrition in younger cohorts who may abandon a survey that is difficult to navigate on a small screen.

**Dedicate a Section to the Analysis of "Mode Effects"**

*   **Observation:** This project marked a transition from CATI (Wave 1) to CAWI (Wave 2). It is well-documented that this change in data collection "mode" can influence responses, particularly regarding social desirability bias.
*   **Recommendation for WHO:** In the analytical plan for future waves, we recommend including a dedicated section in the final report discussing potential mode effects.

*   Respondents may be more honest about sensitive health issues online than when speaking to a human interviewer.
*   This analysis should specifically examine shifts in ‘health system confidence scores’ among older adults, transparency analyzing if changes are due to genuine sentiment shifts or the removal of the "interviewer effect."

**Integrate a Strategic Qualitative Component**

*   **Observation:** The quantitative data provides a robust measure of _what_ the public perceives. However, understanding the underlying reasons and personal experiences, _the why_, is critical for crafting effective policy interventions.
*   **Recommendation for WHO:** We strongly recommend including a small, integrated qualitative component (10-15 in-depth interviews) in the next wave. These interviews should target respondents from key segments (e.g., those reporting forgone care or very low trust in the health system). This would provide powerful, illustrative narratives to enrich the final report and offer deeper context for sensitive areas.

By incorporating these recommendations into the design of future PVS waves, we are confident that the WHO can continue to build upon this project's success, yielding even more efficient, robust, and impactful insights into the healthcare landscape of Romania.

# **8\. Final Deliverables**

A concluding section that lists all materials provided to the client, formally closing the project loop.

In fulfillment of the project requirements, MKOR has provided the World Health Organization with the following final deliverables:

*   **Final Dataset**: Cleaned, validated and weighted datasets in XLS and Stata format, including weighted datasets for the adult sample (N=2,000) and the adolescent oversample (N=500) and a combined weighted dataset for the full sample (N=2,500).
*   **Codebook**: A comprehensive codebook detailing all variables, value labels and definitions for the provided datasets.
*   **Final Technical Report**: This comprehensive document.

# **9\. Project Team and Roles**

To ensure a rigorous and well-managed research process, specific roles and responsibilities were assigned to a team of dedicated experts. The clear delineation of tasks guaranteed accountability and expertise at every stage of the project.

*   **Overall Project Management & Client Liaison**: **Corina Cimpoca, MBA** (Senior Project Manager) was responsible for the strategic oversight of the project, serving as the primary point of contact for the WHO team and ensuring all project milestones and deliverables were met in accordance with the terms of reference.
*   **Methodological Design & Weighting**: **Dr. Mircea Comșa** (Lead Technical Expert) designed the sampling plan and the post-stratification weighting methodology. He supervised the data cleaning process and was responsible for the final calculation of weights and ensuring the statistical representativeness of the dataset.
*   **Fieldwork Management & Data Collection**: **Alexandru Cimpoca** (Data Collection Officer) managed all operational aspects of the fieldwork, including the survey deployment, real-time quota monitoring and the implementation of the specialized outreach strategy for the adolescent oversample.
*   **Data Validation and Quality Control**: The data validation process was led by **Dr. Bianca Balea** (Lead Methodologist & Statistician), who defined the validation criteria. The data processing team conducted manual checks and the final review of flagged responses under her direct supervision.
*   **Reporting and Analysis**: The final analysis and reporting were led by the **senior research team**, ensuring that the findings were interpreted correctly and presented clearly in this technical document.

# **10\. Our Standards and Accreditations**

MKOR’ standards and accreditations provide our clients with the peace of mind that they can always depend on us to deliver reliable, sustainable findings. Our focus on quality and continuous improvement means we have embedded a “right first time” approach throughout our organisation.

**ESOMAR Compliance**

As a member of ESOMAR, the global association for market, social, and opinion research, MKOR adheres to the ICC/ESOMAR International Code on Market, Opinion and Social Research and Data Analytics. This ensures our processes meet the highest global standards for ethical and professional conduct.

**Technology and Tools**

To ensure a secure and efficient research process, the following technology and tools were utilized:

**Data Collection Platform**: All data was collected via MKOR's proprietary, secure survey platform, which allows for real-time monitoring and automated quality checks.

**Database Construction**: The final datasets were constructed and cleaned using STATA Statistics, ensuring data integrity and allowing for the application of complex weighting algorithms.

**ISO 9001:2015**

This is the international general company standard with a focus on continual improvement through quality management systems. In 1994, we became one of the early adopters of the ISO 9001 business standard.

**GDPR Compliance**

MKOR is required to comply with the EU General Data Protection Regulation (GDPR) and the EU Data Protection Act (DPA) 2018. It covers the processing of personal data and the protection of privacy.

# **11\. References**

American Association for Public Opinion Research. (2023). Standard definitions: Final dispositions of case codes and outcome rates for surveys (10th ed.). AAPOR. https://www.aapor.org/Standards-Ethics/Standard-Definitions-(1).aspx

Arnardóttir, J. R., Thoroddsen, A., & Einarsdóttir, J. (2023). Ethical and practical challenges in adolescent survey research: Participation, consent, and response quality. _Child Indicators Research, 16(3)_, 987–1004. https://doi.org/10.1007/s12187-022-09940-9

Baker, R., Blumberg, S. J., Brick, J. M., Couper, M. P., Courtright, M., Dennis, J. M., Dillman, D., Frankel, M. R., Garland, P., Groves, R. M., Kennedy, C., Krosnick, J., Lavrakas, P. J., Lee, S., Link, M., Piekarski, L., Rao, K., Thomas, R. K., & Zahs, D. (2010). _AAPOR report on online panels_. American Association for Public Opinion Research.

Cavazos-Rehg, P. A., Krauss, M. J., Fisher, S. L., Salyer, P., Grucza, R. A., & Bierut, L. J. (2020). Recruiting and retaining adolescents in web-based surveys: Lessons learned from longitudinal studies. _Journal of Adolescent Health_, _66(5)_, 593–600. https://doi.org/10.1016/j.jadohealth.2019.11.305

de Leeuw, E. D. (2012). Surveying children and adolescents. In L. Gideon (Ed.), _Handbook of survey methodology for the social sciences_ (pp. 1–22). Springer.

Dutwin, D. (2023). Reducing coverage bias in Internet surveys. _Social Science Computer Review_.

Gaia, A. (2025). Internet coverage bias in web surveys in Europe. _Survey Research Methods_.

National Institute of Statistics (INS). (2025). Resident population of Romania by age, sex and residence. https://insse.ro

Totura, C. M. W., Karver, M. S., & Gewirtz, A. H. (2017). Recruitment and retention of adolescents in prevention research: A review of strategies. _Journal of Child and Family Studies_, _26(4)_, 945–956. https://doi.org/10.1007/s10826-016-0626-6

Vogel, E. A., Ramo, D. E., Rubinstein, M. L., Delucchi, K. L., Darrow, S., Costello, C., & Prochaska, J. J. (2020). Effects of social media on adolescents’ willingness to participate in health research. _Journal of Medical Internet Research_, _22(5)_, e16228. https://doi.org/10.2196/16228

Waechter, S., Lechner, C. M., & Danner, D. (2023). Challenges in recruiting adolescents for online surveys: Evidence from large-scale panel studies. _Methodology_, _19(1)_, 1–12. https://doi.org/10.5964/meth.9051

World Health Organization. (2014). _Health for the world’s adolescents: A second chance in the second decade_. WHO Press. https://www.who.int/publications/i/item/WHO-FWC-MCA-14.05

# **Appendix 1 - Small-Scale Pilot Survey**

| **Country** | **Pilot start date** | **Pilot end date** |
| --- | --- | --- |
| Romania | 2025-11-02 | 2025-11-04 |

| **Fieldwork Metric** | **Value** | **Description** |
| --- | --- | --- |
| Invitations Sent | 1,480 | The total number of unique invitations required to achieve the pilot target. |
| Survey Starts | 110 | The number of individuals who began the questionnaire. |
| Break-offs (Drop-Offs) | 4 | Respondents who started but did not complete the survey |
| Gross Completes  | 106 | The number of respondents who completed the survey before quality checks. |
| Quality Control Invalidations | 6 | Responses invalidated due to logical incoherence or quality flags. |
| **Final Valid Completes** | **100** | The number of survey respondents remaining after all data quality protocols were applied. |

**Distribution of Pilot Sample**

| **Demographic** | **Value** | **Observations** |
| --- | --- | --- |
| Gender | 74% Women 25% Men 1% Other | The sample skewed significantly female |
| Age | 15-17 yo (Alpha): 1% 18-27 yo (GenZ): 9% 28-43 yo (Millennials): 34% 44-56 yo (Gen X): 40%  57 yo and over (Baby Boomers): 16% | The adolescents sample underrepresented  |

**Length of Survey (LoS)**

| **Fieldwork Metric** | **Value (minutes)** | **Description** |
| --- | --- | --- |
| Average LoS | 36 | The average time it took for someone to respond |
| Average LoS excluding outliers | 17 | The average time it took for someone to respond, excluding people that took more than one hour to respond |
| Median LoS excluding outliers | 15 | The middle value of all valid survey completion times, excluding people that took more than one hour to respond2.2 Methodology |
| Longest active completion time | 50 | The longest survey completion time under one hour, excluding people that took more than one hour to respond |
| Longest total completion time | 1229 | The longest survey completion time for someone that stopped answering and resumed at a later time  |

**Satisfaction Results in Pilot Sample Survey**






# **Appendix 2 - Adults Sample**

**Fieldwork & Sampling Approach**

| **Sample** | **Start date** | **End date** |
| --- | --- | --- |
| Adults | 2025-11-21 | 2025-12-04 |




| **Fieldwork Metric** | **Value** | **Description** |
| --- | --- | --- |
| Total Invitations Sent (including reminders) | 33,501 | The total number of emails sent, including initial invitations and one reminder per non-respondent. | The total number of emails sent, including initial invitations and one reminder per non-respondent. |
| Unique Individuals Invited | 22,684 | The number of unique panelists who received an invitation. | The number of unique panelists who received an invitation. |
| Starts (AAPOR Term: I+P+R) | 2,874 | The number of individuals who clicked the survey link and began the questionnaire. | The number of individuals who clicked the survey link and began the questionnaire. |
| Break-offs / Incompletes (R) | 616 | The number of respondents who started the survey but did not complete it. | The number of respondents who started the survey but did not complete it. |
| Quality Control Invalidations | 258 | The number of respondents who completed the survey but were invalidated during the data cleaning process (for speeding, logical incoherence, etc.). | The number of respondents who completed the survey but were invalidated during the data cleaning process (for speeding, logical incoherence, etc.). |
| **Final Valid Completes (I)** | **2,000** | The number of surveys that were fully completed and passed all data quality validation checks. | The number of surveys that were fully completed and passed all data quality validation checks. |
| Drop-out Rate | 21.4% | The percentage of respondents who started the survey but did not successfully complete it (Break-offs / Starts). | The percentage of respondents who started the survey but did not successfully complete it (Break-offs / Starts). |
| AAPOR Response Rate #3 (RR3) | 8.8% | Final Valid Completes (2,000) / Unique Individuals Invited (22,684). This implies an eligibility rate of 100% for the invited panel sample. | Final Valid Completes (2,000) / Unique Individuals Invited (22,684). This implies an eligibility rate of 100% for the invited panel sample. |
| **Non-Response Bias Analysis: Invited Sample vs. Final Unweighted Sample** |
| **Demographic** | **Invited** | **Valid** (unweighted) |
| **Sample** | **N=** | **22,684** | **2,000** |
| **Gender** | Male | 43.7% | 39.2% |
| Female | 56.3% | 60.6% |
| **Age Group** | 18-34 | 35.0% | 46.0% |
| 35-49 | 39.7% | 23.8% |
| 50-64 | 21.7% | 17.2% |
| 65+ | 3.6% | 4.5% |
| **Education** | High | 36.3% | 42.3% |
| Medium | 41.0% | 38.5% |
| Low | 22.7% | 18.9% |
| **Residence** | Urban | 71.0% | 73.5% |
| Rural | 29.0% | 26.5% |




**Sample: Unweighted, Weighted and INS Target Comparison**

The following table illustrates the demographic profile of the final adult sample before and after weighting, demonstrating the adjustments made to achieve national representativeness.

| **Adult** **Demographic Variable** | **Unweighted Sample (%)** | **Weighted Sample (%)** | **INS Population Target (%)** |
| --- | --- | --- | --- |
| **Gender** |
| Male | 39.2% | 48.8% | 49.2% |
| Female | 60.6% | 51.0% | 50.7% |
| **Age Group** |
| 18-34 | 46.0% | 23.7% | 23.0% |
| 35-49 | 23.8% | 27.4% | 27.0% |
| 50-64 | 17.2% | 25.4% | 25.0% |
| 65+ | 4.5% | 23.5% | 25.0% |
| **Residence** |
| Urban | 73.5% | 56.0% | 56.0% |
| Rural | 26.5% | 44.0% | 44.0% |
| **Education** |
| High | 42.3% | 20.6% | 20.0% |
| Medium | 38.5% | 33.8% | 34.0% |
| Low | 18.9% | 45.6% | 46.0% |




# **Appendix 4 - Combined**

**Fieldwork & Sampling Approach**

| **Sample** | **Start date** | **End date** |
| --- | --- | --- |
| Adolescents | 2025-11-21 | 2025-12-16 |

**Sample: Unweighted, Weighted**

The following table illustrates the demographic profile of the final combined 2,500 sample before and after weighting, demonstrating the adjustments made to achieve national representativeness.

| **Sample** | **Key** | **Value** | **Raw Responses** | **Raw Distribution** | **Wins Distribution** | **Winsx Distribution** |
| --- | --- | --- | --- | --- | --- | --- |
| 18+ and 15-17 | educ | (post)liceu | 790 | 31.60% | 27.22% | 34.00% |
| 18+ and 15-17 | educ | < liceu | 679 | 27.16% | 56.27% | 46.00% |
| 18+ and 15-17 | educ | superioare | 1031 | 41.24% | 16.50% | 20.00% |
| 18+ and 15-17 | mediu | rural | 640 | 25.60% | 45.19% | 48.45% |
| 18+ and 15-17 | mediu | urban | 1860 | 74.40% | 54.81% | 51.55% |
| 18+ and 15-17 | q3 | Another | 4 | 0.16% | 0.27% | 0.05% |
| 18+ and 15-17 | q3 | Man | 1004 | 40.16% | 49.19% | 48.10% |
| 18+ and 15-17 | q3 | Woman | 1492 | 59.68% | 50.54% | 51.86% |
| 18+ and 15-17 | q5 | City | 1860 | 74.40% | 54.81% | 51.55% |
| 18+ and 15-17 | q5 | Village | 640 | 25.60% | 45.19% | 48.45% |
| 18+ and 15-17 | sample | 15-19 | 500 | 20.00% | 20.00% | 5.90% |
| 18+ and 15-17 | sample | 18+ | 2000 | 80.00% | 80.00% | 94.10% |
| 18+ and 15-17 | sex | bărbat | 1008 | 40.32% | 49.46% | 48.14% |
| 18+ and 15-17 | sex | femeie | 1492 | 59.68% | 50.54% | 51.86% |
| 18+ and 15-17 | sexmediu | Female + Rural | 387 | 15.48% | 24.20% | 24.20% |
| 18+ and 15-17 | sexmediu | Female + Urban | 1105 | 44.20% | 26.34% | 27.66% |
| 18+ and 15-17 | sexmediu | Male + Rural | 253 | 10.12% | 20.99% | 24.25% |
| 18+ and 15-17 | sexmediu | Male + Urban | 755 | 30.20% | 28.47% | 23.89% |
| 18+ and 15-17 | varstac | 15-19 | 606 | 24.24% | 28.33% | 6.85% |
| 18+ and 15-17 | varstac | 20-24 | 295 | 11.80% | 5.33% | 6.23% |
| 18+ and 15-17 | varstac | 25-34 | 352 | 14.08% | 5.29% | 12.71% |
| 18+ and 15-17 | varstac | 35-44 | 337 | 13.48% | 11.35% | 16.48% |
| 18+ and 15-17 | varstac | 45-54 | 477 | 19.08% | 19.22% | 18.32% |
| 18+ and 15-17 | varstac | 55-64 | 294 | 11.76% | 11.72% | 15.34% |
| 18+ and 15-17 | varstac | 65+ | 139 | 5.56% | 18.77% | 24.06% |

# **Appendix 4 - Combined Dataset & Weighting Specifications**

**Fieldwork & Sampling Approach**

| **Sample** | **Start date** | **End date** |
| --- | --- | --- |
| Combined (Adults + Adolescents) | 2025-11-21 | 2025-12-16 |

### **Combined Dataset Weighting Variables**

The unified dataset (N=2,500) contains two distinct weighting variables to allow for different types of analysis. 

The weighting scheme used Sex\*Residence (Female/Male + Urban/Rural), Age groups (15-19, 20-24, 25-34, 35-44, 45-54, 55-64, 65+), and Education, calibrated against INS 2025 resident population data.

*   **wins** (Sample-Specific Weight): This variable applies the specific calibration weights calculated for the two distinct samples (Adults N=2,000 and Adolescents N=500) independently.

*   _Usage:_ Use this variable when analyzing the two groups separately within the same file, or when the intention is to maintain the boosted sample size of adolescents for comparative purposes.
*   _Note:_ In this weighting scheme, the proportion of adolescents remains at the oversampled rate (20% of the total sample).

*   **winsx** (Total Population Weight): This variable adjusts the entire combined sample to reflect the actual demographic reality of the Romanian population aged 15+.

*   _Usage:_ Use this variable when analyzing the total N=2,500 as a single, nationally representative entity.
*   _Methodological Note:_ Because adolescents were significantly oversampled (representing 20% of the raw sample versus approx. 6% of the actual population), this weighting variable corrects this disproportion. Consequently, broader limits for minimum and maximum weights (0.14 to 7) were applied to ensure statistical balancing.

### **Sample Structure Comparison**

The following table details the demographic profile of the final combined sample (N=2,500) across three dimensions:

*   the Raw (Unweighted) distribution, 
*   the Wins (Sample-Specific) distribution, and 
*   the Winsx (Total Population) distribution.

| **Demographic Variable** | **Category** | **Raw Distribution (Unweighted)** | **Wins Distribution (Sample Specific)** | **Winsx Distribution (Total Population)** |
| --- | --- | --- | --- | --- |
| Sample Split | Adolescents (15-19) | 20.00% | 20.00% | 5.90% |
|   | Adults (18+) | 80.00% | 80.00% | 94.10% |
| Gender | Male | 40.32% | 49.46% | 48.14% |
|   | Female | 59.68% | 50.54% | 51.86% |
| Residence | Urban | 74.40% | 54.81% | 51.55% |
|   | Rural | 25.60% | 45.19% | 48.45% |
| Education | Low (< High School) | 27.16% | 56.27% | 46.00% |
|   | Medium (High School/Post-Sec) | 31.60% | 27.22% | 34.00% |
|   | High (University+) | 41.24% | 16.50% | 20.00% |
| Age Groups | 15-19 | 24.24% | 28.33% | 6.85% |
|   | 20-24 | 11.80% | 5.33% | 6.23% |
|   | 25-34 | 14.08% | 5.29% | 12.71% |
|   | 35-44 | 13.48% | 11.35% | 16.48% |
|   | 45-54 | 19.08% | 19.22% | 18.32% |
|   | 55-64 | 11.76% | 11.72% | 15.34% |
|   | 65+ | 5.56% | 18.77% | 24.06% |